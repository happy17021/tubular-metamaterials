{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b1593-4faf-499f-9c19-19dfc1db03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, Conv1D, LayerNormalization,\n",
    "    MultiHeadAttention, GlobalAveragePooling1D, Add\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import Hyperband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d66cc-af6e-4c83-a58a-9451362a9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.read_excel('images_index.xlsx') \n",
    "param_df.set_index('name', inplace=True)\n",
    "\n",
    "\n",
    "curve_folder = './curves'  \n",
    "all_curves = {}\n",
    "for file in glob(os.path.join(curve_folder, '*.xlsx')):\n",
    "    df = pd.read_excel(file)\n",
    "    for col in df.columns[1:]:  \n",
    "        if df[col].isnull().any():\n",
    "            continue\n",
    "        all_curves[col] = df[col].values[:200]  \n",
    "\n",
    "X, Y = [], []\n",
    "for name in param_df.index:\n",
    "    if name in all_curves:\n",
    "        X.append(all_curves[name])\n",
    "        Y.append(param_df.loc[name].values)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(f'numb：{len(X)}')\n",
    "\n",
    "scaler_X = StandardScaler().fit(X)\n",
    "scaler_Y = StandardScaler().fit(Y)\n",
    "X_scaled = scaler_X.transform(X)[..., np.newaxis]\n",
    "Y_scaled = scaler_Y.transform(Y)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler_X, 'scaler_X.pkl')\n",
    "joblib.dump(scaler_Y, 'scaler_Y.pkl')\n",
    "\n",
    "print(\"ok\")\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_scaled, Y_scaled, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a5373-47d3-4f13-b8cd-7e9d1c51dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurveHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        inp = Input(shape=(200, 1))\n",
    "\n",
    "        \n",
    "        num_cnn_layers = hp.Int('cnn_layers', 1, 3)\n",
    "        x = inp\n",
    "        for i in range(num_cnn_layers):\n",
    "            filters = hp.Choice(f'filters_{i}', [32, 64, 128])\n",
    "            kernel_size = hp.Choice(f'kernel_{i}', [3, 5])\n",
    "            activation = hp.Choice(f'act_cnn_{i}', ['relu', 'tanh'])\n",
    "            x = Conv1D(filters, kernel_size, activation=activation, padding='same')(x)\n",
    "\n",
    "        \n",
    "        pos_embed = tf.keras.layers.Embedding(input_dim=200, output_dim=x.shape[-1])(tf.range(0, 200))\n",
    "        x = x + pos_embed\n",
    "\n",
    "        \n",
    "        attn = MultiHeadAttention(\n",
    "            num_heads=hp.Choice('num_heads', [2, 4, 8]),\n",
    "            key_dim=hp.Choice('key_dim', [16, 32])\n",
    "        )(x, x)\n",
    "        x = Add()([x, attn])\n",
    "        x = LayerNormalization()(x)\n",
    "\n",
    "        # FFN\n",
    "        ffn_units = hp.Choice('ffn_units', list(range(96, 257, 16)))\n",
    "        act_ffn = hp.Choice('act_ffn', ['relu', 'tanh'])\n",
    "        ffn = Dense(ffn_units, activation=act_ffn)(x)\n",
    "        ffn = Dense(x.shape[-1])(ffn)\n",
    "        x = Add()([x, ffn])\n",
    "        x = LayerNormalization()(x)\n",
    "\n",
    "        # MLP Head\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dropout(hp.Float('dropout1', 0.1, 0.5))(x)\n",
    "        dense_units = hp.Choice('dense_units', list(range(96, 257, 16)))\n",
    "        act_dense = hp.Choice('act_dense', ['relu', 'tanh'])\n",
    "        x = Dense(dense_units, activation=act_dense)(x)\n",
    "        x = Dropout(hp.Float('dropout2', 0.1, 0.5))(x)\n",
    "        out = Dense(10)(x)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=out)\n",
    "        lr = hp.Float('learning_rate', 1e-4, 5e-3, sampling='log')\n",
    "        model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "\n",
    "tuner = Hyperband(\n",
    "    CurveHyperModel(),\n",
    "    objective='val_loss',\n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='tuner_dir',\n",
    "    project_name='cnn_transformer_tune'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "\n",
    "tuner.search(X_train, Y_train,\n",
    "             validation_split=0.1,\n",
    "             epochs=100,\n",
    "             batch_size=32,\n",
    "             callbacks=[early_stop])\n",
    "\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"best：\")\n",
    "for key, value in best_hp.values.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "best_model.save('best_model.h5')\n",
    "print(\"save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6396e-0f5c-4522-9ed5-ff30ae63c56f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = tuner.get_best_models(1)[0]\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239158e-e9a2-4b84-aca3-52816fc136ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "Y_pred_real = scaler_Y.inverse_transform(Y_pred)\n",
    "Y_test_real = scaler_Y.inverse_transform(Y_test)\n",
    "\n",
    "Y_pred_rounded = np.round(Y_pred_real)\n",
    "Y_pred_rounded[:, 0:8] = np.clip(Y_pred_rounded[:, 0:8], 4, 13)      \n",
    "Y_pred_rounded[:, 8] = np.clip(Y_pred_rounded[:, 8], 16, 112)         \n",
    "Y_pred_rounded[:, 9] = np.clip(Y_pred_rounded[:, 9], 16, 112)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e4e50-7ec9-4273-9e03-d5a925bd5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "param_names = ['top','left','bottom','right','diag1','diag2','vert','hori','x','y']\n",
    "for i, name in enumerate(param_names):\n",
    "    r2 = r2_score(Y_test_real[:, i], Y_pred_rounded[:, i])\n",
    "    mae = mean_absolute_error(Y_test_real[:, i], Y_pred_rounded[:, i])\n",
    "    acc = np.mean(Y_test_real[:, i] == Y_pred_rounded[:, i]) * 100\n",
    "    print(f\"{name:<6} | R²: {r2:.3f} | MAE: {mae:.2f} | acc: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92ed7a-abd0-4c93-90c0-9fc1f639e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

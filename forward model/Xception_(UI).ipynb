{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f7dee-59fd-4179-9df5-8e7de92e7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import keras_tuner as kt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tf.random.set_seed(20)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a24a3-d739-4f78-be6f-0777db93dd1f",
   "metadata": {},
   "source": [
    "## 切分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d1201-1de2-42c1-9858-1740233a0470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "CURVES_DIR        = './curves'           \n",
    "IMAGES_INDEX_XLSX = 'images_index.xlsx' \n",
    "IMAGES_ROOT       = './images'           \n",
    "TRAIN_IMG_DIR     = './train_images'     \n",
    "TEST_IMG_DIR      = './test_images'     \n",
    "\n",
    "\n",
    "USE_STEM_MATCH = True\n",
    "\n",
    "\n",
    "labels = pd.DataFrame(columns=['name'] + [f'y{i}' for i in range(200)])\n",
    "\n",
    "for file in tqdm(os.listdir(CURVES_DIR), desc=\"Collect curves\"):\n",
    "    if not file.lower().endswith(('.xlsx', '.xls', '.csv')):\n",
    "        continue\n",
    "    fp = os.path.join(CURVES_DIR, file)\n",
    "    if file.lower().endswith('.csv'):\n",
    "        tmp = pd.read_csv(fp, index_col=0).iloc[1:, ].T\n",
    "    else:\n",
    "        tmp = pd.read_excel(fp, index_col=0).iloc[1:, ].T\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp.columns = ['name'] + [f'y{i}' for i in range(200)]\n",
    "    labels = pd.concat([labels, tmp], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "labels2 = pd.read_excel(IMAGES_INDEX_XLSX)\n",
    "\n",
    "\n",
    "def stem(s):\n",
    "    try:\n",
    "        base = os.path.basename(str(s))\n",
    "        return os.path.splitext(base)[0]\n",
    "    except:\n",
    "        return str(s)\n",
    "\n",
    "if USE_STEM_MATCH:\n",
    "    labels['name_stem']  = labels['name'].apply(stem)\n",
    "    labels2['name_stem'] = labels2['name'].apply(stem)\n",
    "    \n",
    "    labels = pd.merge(labels2[['name', 'name_stem']], labels, how='inner', on='name_stem')\n",
    " \n",
    "    labels.drop(columns=['name_x'], inplace=True)\n",
    "    labels.rename(columns={'name_y': 'name'}, inplace=True)\n",
    "    labels.drop(columns=['name_stem'], inplace=True)\n",
    "else:\n",
    "    labels = pd.merge(labels2[['name']], labels, how='inner', on='name')\n",
    "\n",
    "\n",
    "y_cols = [f'y{i}' for i in range(200)]\n",
    "ss = StandardScaler()\n",
    "labels[y_cols] = ss.fit_transform(labels[y_cols])\n",
    "joblib.dump(ss, 'ss.pkl')\n",
    "\n",
    "\n",
    "def list_files(d):\n",
    "    return sorted([f for f in os.listdir(d) if os.path.isfile(os.path.join(d, f))])\n",
    "\n",
    "train_files = set(list_files(TRAIN_IMG_DIR))\n",
    "test_files  = set(list_files(TEST_IMG_DIR))\n",
    "\n",
    "if USE_STEM_MATCH:\n",
    "  \n",
    "    train_map = {stem(f): f for f in train_files}\n",
    "    test_map  = {stem(f): f for f in test_files}\n",
    "\n",
    "\n",
    "    train_mask = labels['name'].apply(lambda x: stem(x) in train_map)\n",
    "    train_csv  = labels[train_mask].copy().reset_index(drop=True)\n",
    "    train_csv['name'] = train_csv['name'].apply(lambda x: train_map.get(stem(x), x))\n",
    "\n",
    "\n",
    "    test_mask = labels['name'].apply(lambda x: stem(x) in test_map)\n",
    "    test_csv  = labels[test_mask].copy().reset_index(drop=True)\n",
    "    test_csv['name'] = test_csv['name'].apply(lambda x: test_map.get(stem(x), x))\n",
    "\n",
    "\n",
    "else:\n",
    "  \n",
    "    train_csv = labels[labels['name'].isin(train_files)].reset_index(drop=True)\n",
    "    test_csv  = labels[labels['name'].isin(test_files)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_csv)}, Test samples: {len(test_csv)}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4), dpi=120)\n",
    "for i in range(min(10, len(labels))):\n",
    "    y_values = labels.loc[i, y_cols].values\n",
    "    sns.lineplot(x=range(200), y=y_values)\n",
    "plt.title(\"Sample of standardized curves\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347bf435-7477-4a9b-864f-ec351e6e8139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, csv_file, directory, batch_size, target_size, y_cols,\n",
    "                 shuffle=True, augment=False):\n",
    "        self.csv_file = csv_file\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.y_cols = y_cols\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        if self.augment:\n",
    "          \n",
    "            self.idg = ImageDataGenerator(\n",
    "                rotation_range=10,\n",
    "                width_shift_range=0.1,\n",
    "                height_shift_range=0.1,\n",
    "                shear_range=0.05,\n",
    "                zoom_range=0.1,\n",
    "                horizontal_flip=True,\n",
    "                fill_mode='nearest'\n",
    "            )\n",
    "        else:\n",
    "            self.idg = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.csv_file) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.csv_file.iloc[k] for k in indexes]\n",
    "        X, y = self.__data_generation(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.csv_file))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch):\n",
    "        X = np.empty((self.batch_size, *self.target_size, 3), dtype=np.float32)\n",
    "        y = np.empty((self.batch_size, len(self.y_cols)), dtype=np.float32)\n",
    "\n",
    "        for i, data in enumerate(batch):\n",
    "  \n",
    "            img_path = os.path.join(self.directory, data['name'])\n",
    "            image = load_img(img_path, target_size=self.target_size)\n",
    "            arr = img_to_array(image) / 255.0\n",
    "\n",
    "            if self.augment and self.idg is not None:\n",
    "                arr = self.idg.random_transform(arr)\n",
    "\n",
    "            X[i] = arr\n",
    "            y[i] = data[self.y_cols].values.astype(np.float32)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "batch_size  = 16\n",
    "target_size = (256, 256)\n",
    "\n",
    "train_generator = CustomDataGenerator(\n",
    "    csv_file=train_csv, directory=TRAIN_IMG_DIR,\n",
    "    batch_size=batch_size, target_size=target_size, y_cols=y_cols,\n",
    "    shuffle=True, augment=True\n",
    ")\n",
    "test_generator = CustomDataGenerator(\n",
    "    csv_file=test_csv, directory=TEST_IMG_DIR,\n",
    "    batch_size=batch_size, target_size=target_size, y_cols=y_cols,\n",
    "    shuffle=False, augment=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89e57b-6a0e-4f0b-9ae2-d98561600613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stack_y(gen):\n",
    "    ys = []\n",
    "    for i in range(len(gen)):\n",
    "        _, yy = gen[i]\n",
    "        ys.append(yy)\n",
    "    return np.concatenate(ys, axis=0)\n",
    "\n",
    "y_train = stack_y(train_generator)\n",
    "y_test  = stack_y(test_generator)\n",
    "print(\"y_train.shape, y_test.shape =\", y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 3), dpi=120)\n",
    "    axs[0].plot(range(1, len(model_history.history['mse'])+1), model_history.history['mse'])\n",
    "    axs[0].plot(range(1, len(model_history.history['val_mse'])+1), model_history.history['val_mse'])\n",
    "    axs[0].set_title('Model MSE'); axs[0].set_ylabel('mse'); axs[0].set_xlabel('Epoch'); axs[0].legend(['train', 'val'], loc='best')\n",
    "    axs[1].plot(range(1, len(model_history.history['loss'])+1), model_history.history['loss'])\n",
    "    axs[1].plot(range(1, len(model_history.history['val_loss'])+1), model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss'); axs[1].set_ylabel('Loss'); axs[1].set_xlabel('Epoch'); axs[1].legend(['train', 'val'], loc='best')\n",
    "    fig.savefig('curve.jpg', dpi=600, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7f66f-019a-489e-9ae1-6ef7f84b4c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input_image = Input(shape=(256, 256, 3))\n",
    "\n",
    "    base_model = tf.keras.applications.Xception(\n",
    "        weights='imagenet', include_top=False, input_shape=(256, 256, 3)\n",
    "    )\n",
    "    x = base_model(input_image)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "\n",
    "    for i in range(2):\n",
    "        x = Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=128, max_value=512, step=16),\n",
    "            activation=hp.Choice(f'activation_{i}', values=['relu', 'tanh'])\n",
    "        )(x)\n",
    "\n",
    "    output = Dense(200)(x)\n",
    "    model = Model(inputs=input_image, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-3, sampling='LOG')\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "TUNER_DIR = 'my_dir'\n",
    "PROJECT   = 'helloworld_single_image'\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=100,            \n",
    "    executions_per_trial=1,\n",
    "    directory=TUNER_DIR,\n",
    "    project_name=PROJECT\n",
    ")\n",
    "\n",
    "\n",
    "already_tuned = False\n",
    "try:\n",
    "   \n",
    "    _ = tuner.get_best_hyperparameters(num_trials=1)\n",
    "    if len(_) > 0:\n",
    "        already_tuned = True\n",
    "except Exception:\n",
    "    already_tuned = False\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "if not already_tuned:\n",
    "   \n",
    "    tuner.search(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=[stop_early],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"[BEST HP] lr={best_hps.get('learning_rate')}\")\n",
    "for i in range(2):\n",
    "    print(f\"[BEST HP] Dense{i+1}: units={best_hps.get(f'units_{i}')}, act={best_hps.get(f'activation_{i}')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a6cf0-069d-46c0-b8b0-fd9c74119943",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"./best_model.h5\", save_best_only=True, monitor='val_loss', mode='min'\n",
    ")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=100,                              \n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator),\n",
    "    callbacks=[ckpt, stop_early],\n",
    "    verbose=1\n",
    ")\n",
    "plot_model_history(history)\n",
    "\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df.insert(0, 'epoch', range(1, len(hist_df)+1)) \n",
    "\n",
    "\n",
    "hist_df.to_excel('history_epoch_metrics.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0815fda-5674-47c4-b432-6404eb9494ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best = load_model('best_model.h5', compile=False)\n",
    "\n",
    "y_test_pred  = best.predict(test_generator,  verbose=0)\n",
    "y_train_pred = best.predict(train_generator, verbose=0)\n",
    "\n",
    "\n",
    "y_test_true = joblib.load('ss.pkl').inverse_transform(y_test)\n",
    "y_test_pred_is = joblib.load('ss.pkl').inverse_transform(y_test_pred)\n",
    "\n",
    "\n",
    "r2_list, area_list = [], []\n",
    "for i in range(y_test_true.shape[0]):\n",
    "    r2_list.append(round(metrics.r2_score(y_test_true[i, :], y_test_pred_is[i, :]), 4))\n",
    "    denom = np.trapz(y_test_true[i, :], dx=1.0)\n",
    "    nume  = np.trapz(y_test_pred_is[i, :], dx=1.0)\n",
    "    area_list.append((nume / denom) if denom != 0 else np.nan)\n",
    "\n",
    "eps = 1e-8\n",
    "denom_mat = np.maximum(np.abs(y_test_true), eps)  # shape: (N,200)\n",
    "rel_rmse_vec  = np.sqrt(np.mean(((y_test_pred_is - y_test_true) / denom_mat) ** 2, axis=1))\n",
    "rel_rmse_mean = float(np.nanmean(rel_rmse_vec))\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    'name': test_csv['name'].reset_index(drop=True),\n",
    "    'R2': r2_list,\n",
    "    'AreaRatio': area_list,\n",
    "    'RelRMSE': rel_rmse_vec\n",
    "})\n",
    "df_metrics.to_excel('test_metrics_R2_Area_RelRMSE.xlsx', index=False)\n",
    "\n",
    "print(\"Test R² mean:\", np.nanmean(r2_list))\n",
    "print(\"Test AreaRatio mean:\", np.nanmean(area_list))\n",
    "print(\"Test RelRMSE mean:\", rel_rmse_mean)\n",
    "\n",
    "\n",
    "pd.DataFrame(y_test_true).assign(name=test_csv['name'].values).to_excel('True-test.xlsx', index=False)\n",
    "pd.DataFrame(y_test_pred_is).assign(name=test_csv['name'].values).to_excel('Pred-test.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f258-8058-453d-a93b-019a21d47b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "num_epoch = len(loss_values)\n",
    "df_loss = pd.DataFrame({'epoch': np.arange(1, num_epoch + 1),\n",
    "                        'loss': loss_values,\n",
    "                        'val_loss': val_loss_values})\n",
    "df_loss.to_csv('loss_curve_image_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c69474-40a5-4c9a-af73-22ea9a0e0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = min(10, y_test_true.shape[0])\n",
    "for i in range(k):\n",
    "    plt.figure(figsize=(8, 3), dpi=120)\n",
    "    sns.lineplot(x=range(200), y=y_test_true[i, :], label='True')\n",
    "    sns.lineplot(x=range(200), y=y_test_pred_is[i, :], label='Pred')\n",
    "    plt.title(f\"{test_csv.iloc[i]['name']} | R²={r2_list[i]:.3f}, RelRMSE={rel_rmse_vec[i]:.3f}\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4df95-4fc2-465e-ace6-f007f7033bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_y_and_names(generator, csv_df):\n",
    "    y_all = []\n",
    "    names_all = []\n",
    "    for bi in range(len(generator)):\n",
    "        Xb, yb = generator[bi]\n",
    "        y_all.append(yb)\n",
    "\n",
    "        start = bi * generator.batch_size\n",
    "        end = min((bi + 1) * generator.batch_size, len(csv_df))\n",
    "        batch_names = list(csv_df.iloc[start:end]['name'].values)\n",
    "        names_all.extend(batch_names)\n",
    "    return np.concatenate(y_all, axis=0), names_all\n",
    "\n",
    "y_train, names_train = collect_y_and_names(train_generator, train_csv)\n",
    "y_test,  names_test  = collect_y_and_names(test_generator,  test_csv)\n",
    "\n",
    "\n",
    "y_train_pred = model.predict(train_generator, verbose=0)\n",
    "y_test_pred  = model.predict(test_generator,  verbose=0)\n",
    "\n",
    "\n",
    "ss: StandardScaler = joblib.load('ss.pkl')\n",
    "y_train_inv = ss.inverse_transform(y_train)\n",
    "y_test_inv  = ss.inverse_transform(y_test)\n",
    "y_train_pred_inv = ss.inverse_transform(y_train_pred)\n",
    "y_test_pred_inv  = ss.inverse_transform(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954da29-3e06-48d3-9bf6-51cb3322d7fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "r2_list, area_list = [], []\n",
    "for i in range(y_test_true.shape[0]):\n",
    "    r2_list.append(round(metrics.r2_score(y_test_true[i, :], y_test_pred_is[i, :]), 4))\n",
    "\n",
    "    denom = np.trapz(y_test_true[i, :], dx=1.0)\n",
    "    nume  = np.trapz(y_test_pred_is[i, :], dx=1.0)\n",
    "    area_list.append((nume / denom) if denom != 0 else np.nan)\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "denom = np.maximum(np.abs(y_test_true), eps)              # shape: (N,200)\n",
    "rel_rmse_vec = np.sqrt(np.mean(((y_test_pred_is - y_test_true) / denom) ** 2, axis=1))\n",
    "rel_rmse_mean = float(np.nanmean(rel_rmse_vec))\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    'name': test_csv['name'].reset_index(drop=True),\n",
    "    'R2': r2_list,\n",
    "    'AreaRatio': area_list,\n",
    "    'RelRMSE': rel_rmse_vec\n",
    "})\n",
    "df_metrics.to_excel('test_metrics_R2_Area_RelRMSE.xlsx', index=False)\n",
    "\n",
    "print(\"Test R² mean:\", np.nanmean(r2_list))\n",
    "print(\"Test AreaRatio mean:\", np.nanmean(area_list))\n",
    "print(\"Test RelRMSE mean:\", rel_rmse_mean)\n",
    "\n",
    "\n",
    "pd.DataFrame(y_test_true).assign(name=test_csv['name'].values).to_excel('True-test.xlsx', index=False)\n",
    "pd.DataFrame(y_test_pred_is).assign(name=test_csv['name'].values).to_excel('Pred-test.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0111007-c5ca-4d90-aac3-98f570ed41c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

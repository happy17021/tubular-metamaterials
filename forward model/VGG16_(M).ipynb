{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc1a385-ca0e-4a57-a3ff-c8a1009151e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "tensorflow = tf \n",
    "tf.random.set_seed(20)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Flatten, Input, Concatenate,\n",
    "    GlobalAveragePooling2D, Lambda, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228962c0-2412-4615-a4d3-8de859682ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURVES_DIR       = './curves'          \n",
    "IMAGES_INDEX_XLSX= 'images_index.xlsx' \n",
    "IMAGES_ROOT      = './images'          \n",
    "TRAIN_IMG_DIR    = './train_images'  \n",
    "TEST_IMG_DIR     = './test_images'   \n",
    "\n",
    "\n",
    "labels = pd.DataFrame(columns=['name'] + ['y' + str(i) for i in range(200)])\n",
    "for file in tqdm(os.listdir(CURVES_DIR), desc='Reading curves'):\n",
    "    if not file.lower().endswith(('.xlsx', '.xls', '.csv')):\n",
    "        continue\n",
    "    tmp = pd.read_excel(f'{CURVES_DIR}/{file}', index_col=0).iloc[1:, ].T\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp.columns = ['name'] + ['y' + str(i) for i in range(200)]\n",
    "    labels = pd.concat([labels, tmp], axis=0)\n",
    "\n",
    "labels2 = pd.read_excel(IMAGES_INDEX_XLSX) \n",
    "labels = pd.merge(labels2, labels, how='inner', on='name').reset_index(drop=True)\n",
    "\n",
    "ss = StandardScaler()\n",
    "y_cols = ['y' + str(i) for i in range(200)]\n",
    "labels[y_cols] = ss.fit_transform(labels[y_cols])\n",
    "joblib.dump(ss, 'ss.pkl')\n",
    "\n",
    "\n",
    "def list_names_from_dir(d):\n",
    "    files = [f for f in os.listdir(d) if os.path.isfile(os.path.join(d, f))]\n",
    "    return set(files)\n",
    "\n",
    "train_names = list_names_from_dir(TRAIN_IMG_DIR)\n",
    "test_names  = list_names_from_dir(TEST_IMG_DIR)\n",
    "\n",
    "\n",
    "train_csv = labels[labels['name'].isin(train_names)].reset_index(drop=True)\n",
    "test_csv  = labels[labels['name'].isin(test_names)].reset_index(drop=True)\n",
    "print(f\"Train samples: {len(train_csv)}, Test samples: {len(test_csv)}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4), dpi=120)\n",
    "for i in range(min(10, len(labels))):\n",
    "    y_values = labels.iloc[i, labels.columns.get_loc('y0'):labels.columns.get_loc('y199')+1].values\n",
    "    sns.lineplot(x=range(200), y=y_values)\n",
    "plt.title(\"curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca5be3-ce85-482c-8d7e-80e4d370b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, csv_file, directory, batch_size, target_size, label_list,\n",
    "                 shuffle=True, augment=False):\n",
    "        self.csv_file = csv_file\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.label_list = label_list\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        if self.augment:\n",
    "            \n",
    "            self.idg = ImageDataGenerator(\n",
    "                rotation_range=5,\n",
    "                width_shift_range=0.1,\n",
    "                height_shift_range=0.1,\n",
    "                shear_range=0.02,\n",
    "                zoom_range=0.08,\n",
    "                horizontal_flip=True,\n",
    "                fill_mode='nearest'\n",
    "            )\n",
    "        else:\n",
    "            self.idg = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.csv_file) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch = [self.csv_file.iloc[k] for k in indexes]\n",
    "        X, y = self.__data_generation(batch)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.csv_file))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch):\n",
    "        X1 = np.empty((self.batch_size, *self.target_size, 3), dtype=np.float32)\n",
    "        X2 = np.empty((self.batch_size, 1), dtype=np.float32)\n",
    "        y  = np.empty((self.batch_size, len(self.label_list) - 1), dtype=np.float32)\n",
    "\n",
    "        for i, data in enumerate(batch):\n",
    "            img_path = os.path.join(self.directory, data['name'])\n",
    "            image = load_img(img_path, target_size=self.target_size)\n",
    "            arr = img_to_array(image) / 255.0\n",
    "            if self.augment and self.idg is not None:\n",
    "                arr = self.idg.random_transform(arr)\n",
    "\n",
    "            X1[i] = arr\n",
    "            X2[i, 0] = np.float32(data[self.label_list[0]])                 \n",
    "            y[i]    = np.asarray(data[self.label_list[1:]], np.float32)     \n",
    "\n",
    "        return [X1, X2], y\n",
    "\n",
    "\n",
    "\n",
    "label_list = ['porosity'] + y_cols\n",
    "batch_size = 16\n",
    "target_size = (256, 256)\n",
    "\n",
    "\n",
    "train_generator = CustomDataGenerator(train_csv, TRAIN_IMG_DIR, batch_size, target_size, label_list,\n",
    "                                      shuffle=True, augment=True)\n",
    "test_generator  = CustomDataGenerator(test_csv,  TEST_IMG_DIR,  batch_size, target_size, label_list,\n",
    "                                      shuffle=False, augment=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8db34-5205-46ac-8e60-80b8aee6adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_y(gen):\n",
    "    ys = []\n",
    "    for _i in range(len(gen)):\n",
    "        _, yy = gen[_i]\n",
    "        ys.append(yy)\n",
    "    return np.concatenate(ys, axis=0)\n",
    "\n",
    "y_train = stack_y(train_generator)\n",
    "y_test  = stack_y(test_generator)\n",
    "print(\"y_train.shape, y_test.shape =\", y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 3), dpi=120)\n",
    "    axs[0].plot(range(1, len(model_history.history['mse'])+1), model_history.history['mse'])\n",
    "    axs[0].plot(range(1, len(model_history.history['val_mse'])+1), model_history.history['val_mse'])\n",
    "    axs[0].set_title('Model MSE'); axs[0].set_ylabel('mse'); axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    axs[1].plot(range(1, len(model_history.history['loss'])+1), model_history.history['loss'])\n",
    "    axs[1].plot(range(1, len(model_history.history['val_loss'])+1), model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss'); axs[1].set_ylabel('Loss'); axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "\n",
    "    fig.savefig('curve.jpg', dpi=600, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef42b6-3c3a-49e6-a6c8-30607bb7c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp: kt.HyperParameters):\n",
    "\n",
    "    input_image     = Input(shape=(256, 256, 3), name='image')\n",
    "    input_porosity  = Input(shape=(1,), name='porosity')\n",
    "\n",
    "\n",
    "    x = Lambda(lambda z: tf.keras.applications.vgg16.preprocess_input(z * 255.0),\n",
    "               name=\"vgg16_preprocess\")(input_image)\n",
    "\n",
    "\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "\n",
    "    head = hp.Choice('head', values=['gap', 'flatten'])\n",
    "    f = base_model(x)\n",
    "    if head == 'gap':\n",
    "        f = GlobalAveragePooling2D(name='gap')(f)\n",
    "    else:\n",
    "        f = Flatten(name='flat')(f)\n",
    "\n",
    "    h = Concatenate(name='concat_feat')([f, input_porosity])\n",
    "    h = LayerNormalization(name='ln_concat')(h)\n",
    "\n",
    "    units1 = hp.Int('units_0', min_value=256, max_value=1024, step=128)\n",
    "    units2 = hp.Int('units_1', min_value=128, max_value=512,  step=64)\n",
    "    act    = hp.Choice('activation', values=['relu', 'tanh'])\n",
    "    l2reg  = hp.Choice('l2', values=[1e-5, 5e-5, 1e-4])\n",
    "    drop1  = hp.Choice('drop1', values=[0.0, 0.2, 0.3])\n",
    "    drop2  = hp.Choice('drop2', values=[0.0, 0.2, 0.3])\n",
    "\n",
    "    h = Dense(units1, activation=act, kernel_regularizer=regularizers.l2(l2reg), name='fc1')(h)\n",
    "    if drop1 > 0: h = Dropout(drop1, name='do1')(h)\n",
    "    h = Dense(units2, activation=act, kernel_regularizer=regularizers.l2(l2reg), name='fc2')(h)\n",
    "    if drop2 > 0: h = Dropout(drop2, name='do2')(h)\n",
    "\n",
    "    output = Dense(200, activation='linear', name='curve_200')(h)\n",
    "\n",
    "    model = Model(inputs=[input_image, input_porosity], outputs=output, name='vgg16_curve_reg')\n",
    "\n",
    "    lr = hp.Float('learning_rate', min_value=1e-5, max_value=5e-4, sampling='LOG')\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=100,              \n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='vgg16_tune'\n",
    ")\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=4,\n",
    "                           restore_best_weights=True)\n",
    "csv_search = CSVLogger('train_log_search.csv', separator=',', append=False)\n",
    "\n",
    "\n",
    "tuner.search(train_generator, epochs=10, validation_data=test_generator, callbacks=[stop_early, csv_search], verbose=1)\n",
    "\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hparams:\",\n",
    "      {k: best_hps.get(k) for k in ['head','units_0','units_1','activation','l2','drop1','drop2','learning_rate']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e707a0-a375-4571-a54b-25587719eca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "ckpt       = ModelCheckpoint(\"./best_model.h5\", save_best_only=True, monitor='val_loss')\n",
    "stop_early_full = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
    "plateau    = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "csv_full   = CSVLogger('train_log_full.csv', separator=',', append=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=100,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator),\n",
    "    callbacks=[ckpt, stop_early_full, plateau, csv_full],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "plot_model_history(history)\n",
    "\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "hist_df.insert(0, 'epoch', range(1, len(hist_df)+1))\n",
    "hist_df.to_excel('history_epoch_metrics.xlsx', index=False)\n",
    "hist_df.to_csv('history_epoch_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9c872-6bd2-4ddb-a116-ab09198072e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = load_model('best_model.h5', compile=False)\n",
    "\n",
    "y_test_pred  = model.predict(test_generator, verbose=0)\n",
    "y_train_pred = model.predict(train_generator, verbose=0)\n",
    "\n",
    "\n",
    "y_test_true    = ss.inverse_transform(y_test)\n",
    "y_test_pred_is = ss.inverse_transform(y_test_pred)\n",
    "\n",
    "\n",
    "r2_list, area_list = [], []\n",
    "for i in range(y_test_true.shape[0]):\n",
    "    r2_list.append(round(metrics.r2_score(y_test_true[i, :], y_test_pred_is[i, :]), 4))\n",
    "    denom = np.trapz(y_test_true[i, :], dx=1.0)\n",
    "    nume  = np.trapz(y_test_pred_is[i, :], dx=1.0)\n",
    "    area_list.append((nume / denom) if denom != 0 else np.nan)\n",
    "\n",
    "\n",
    "eps   = 1e-8\n",
    "denom = np.maximum(np.abs(y_test_true), eps)  # shape: (N,200)\n",
    "rel_rmse_vec  = np.sqrt(np.mean(((y_test_pred_is - y_test_true) / denom) ** 2, axis=1))\n",
    "rel_rmse_mean = float(np.nanmean(rel_rmse_vec))\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame({\n",
    "    'name': test_csv['name'].reset_index(drop=True),\n",
    "    'R2': r2_list,\n",
    "    'AreaRatio': area_list,\n",
    "    'RelRMSE': rel_rmse_vec\n",
    "})\n",
    "df_metrics.to_excel('test_metrics_R2_Area_RelRMSE.xlsx', index=False)\n",
    "print(\"Test R² mean:\", np.nanmean(r2_list))\n",
    "print(\"Test AreaRatio mean:\", np.nanmean(area_list))\n",
    "print(\"Test RelRMSE mean:\", rel_rmse_mean)\n",
    "\n",
    "\n",
    "pd.DataFrame(y_test_true).assign(name=test_csv['name'].values).to_excel('True-test.xlsx', index=False)\n",
    "pd.DataFrame(y_test_pred_is).assign(name=test_csv['name'].values).to_excel('Pred-test.xlsx', index=False)\n",
    "\n",
    "\n",
    "k = min(10, y_test_true.shape[0])\n",
    "for i in range(k):\n",
    "    plt.figure(figsize=(8,3), dpi=120)\n",
    "    sns.lineplot(x=range(200), y=y_test_true[i,:], label='True')\n",
    "    sns.lineplot(x=range(200), y=y_test_pred_is[i,:], label='Pred')\n",
    "    plt.title(f\"{test_csv.iloc[i]['name']} | R²={r2_list[i]:.3f}, RelRMSE={rel_rmse_vec[i]:.3f}\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "Xb, _ = train_generator[0]\n",
    "pred = model.predict(Xb, verbose=0)\n",
    "print(\"Pred std across samples (mean over 200 dims):\", pred.std(axis=0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f3dc4-140f-4f16-862d-afd9037a8900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
